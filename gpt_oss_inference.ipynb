{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e32db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  3 06:46:01 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   45C    P8              24W / 450W |    555MiB / 24564MiB |      6%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2037      G   /usr/lib/xorg/Xorg                          265MiB |\n",
      "|    0   N/A  N/A      2262      G   /usr/bin/gnome-shell                         54MiB |\n",
      "|    0   N/A  N/A      6022      G   ...seed-version=20250831-180037.590000       66MiB |\n",
      "|    0   N/A  N/A     56293      G   /usr/share/code/code                        132MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748108ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0cfe5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d621d0292c184b9bb28f3c177fdd7624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f1b61c622954dfa82f998a98547576f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"openai/gpt-oss-20b\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbfcc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Sep  3 06:46:13 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   47C    P2              68W / 450W |  14142MiB / 24564MiB |      7%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2037      G   /usr/lib/xorg/Xorg                          265MiB |\n",
      "|    0   N/A  N/A      2262      G   /usr/bin/gnome-shell                         54MiB |\n",
      "|    0   N/A  N/A      6022      G   ...seed-version=20250831-180037.590000       66MiB |\n",
      "|    0   N/A  N/A     56293      G   /usr/share/code/code                        138MiB |\n",
      "|    0   N/A  N/A   3949990      C   ...ma/anaconda3/envs/models/bin/python    13576MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea90245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GptOssForCausalLM(\n",
      "  (model): GptOssModel(\n",
      "    (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x GptOssDecoderLayer(\n",
      "        (self_attn): GptOssAttention(\n",
      "          (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
      "          (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
      "        )\n",
      "        (mlp): GptOssMLP(\n",
      "          (router): GptOssTopKRouter()\n",
      "          (experts): Mxfp4GptOssExperts()\n",
      "        )\n",
      "        (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "        (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "    (rotary_emb): GptOssRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pipe.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e352184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([201088, 2880]) torch.bfloat16 579133440\n",
      "model.layers.0.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.0.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.0.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.0.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.0.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.0.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.0.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.0.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.0.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.0.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.1.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.1.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.1.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.1.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.1.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.1.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.1.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.1.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.1.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.1.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.2.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.2.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.2.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.2.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.2.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.2.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.2.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.2.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.2.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.2.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.3.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.3.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.3.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.3.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.3.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.3.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.3.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.3.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.3.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.3.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.4.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.4.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.4.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.4.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.4.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.4.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.4.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.4.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.4.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.4.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.5.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.5.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.5.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.5.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.5.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.5.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.5.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.5.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.5.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.5.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.6.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.6.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.6.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.6.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.6.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.6.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.6.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.6.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.6.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.6.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.7.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.7.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.7.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.7.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.7.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.7.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.7.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.7.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.7.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.7.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.8.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.8.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.8.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.8.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.8.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.8.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.8.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.8.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.8.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.8.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.9.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.9.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.9.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.9.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.9.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.9.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.9.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.9.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.9.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.9.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.10.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.10.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.10.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.10.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.10.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.10.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.10.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.10.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.10.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.10.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.11.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.11.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.11.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.11.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.11.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.11.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.11.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.11.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.11.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.11.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.12.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.12.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.12.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.12.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.12.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.12.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.12.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.12.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.12.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.12.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.13.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.13.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.13.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.13.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.13.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.13.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.13.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.13.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.13.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.13.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.14.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.14.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.14.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.14.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.14.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.14.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.14.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.14.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.14.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.14.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.15.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.15.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.15.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.15.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.15.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.15.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.15.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.15.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.15.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.15.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.16.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.16.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.16.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.16.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.16.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.16.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.16.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.16.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.16.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.16.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.17.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.17.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.17.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.17.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.17.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.17.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.17.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.17.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.17.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.17.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.18.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.18.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.18.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.18.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.18.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.18.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.18.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.18.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.18.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.18.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.19.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.19.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.19.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.19.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.19.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.19.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.19.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.19.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.19.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.19.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.20.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.20.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.20.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.20.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.20.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.20.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.20.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.20.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.20.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.20.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.21.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.21.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.21.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.21.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.21.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.21.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.21.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.21.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.21.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.21.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.22.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.22.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.22.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.22.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.22.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.22.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.22.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.22.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.22.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.22.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.23.self_attn.sinks torch.Size([64]) torch.bfloat16 64\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16 11796480\n",
      "model.layers.23.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16 4096\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.23.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16 1474560\n",
      "model.layers.23.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16 512\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16 11796480\n",
      "model.layers.23.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.23.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.23.mlp.router.bias torch.Size([32]) torch.bfloat16 32\n",
      "model.layers.23.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16 184320\n",
      "model.layers.23.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16 92160\n",
      "model.layers.23.input_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "model.norm.weight torch.Size([2880]) torch.bfloat16 2880\n",
      "lm_head.weight torch.Size([201088, 2880]) torch.bfloat16 579133440\n",
      "Number of parameters: 1804459584\n"
     ]
    }
   ],
   "source": [
    "num_parameters = 0\n",
    "parameters_state_dict = pipe.model.state_dict()\n",
    "for key, value in parameters_state_dict.items():\n",
    "    print(key, value.size(), value.dtype, value.numel())\n",
    "    num_parameters += value.numel()\n",
    "print(f\"Number of parameters: {num_parameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5296cf8e-8893-4e8e-a674-a4ab24cf9d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysisWe need to explain quantum mechanics clearly and concisely. We can aim for a general overview: key principles: wave-particle duality, superposition, uncertainty principle, quantization, measurement, Schrödinger equation, entanglement. Keep concise but clear. Use analogies. Probably 300-500 words. Let's produce.assistantfinal## Quantum Mechanics in a Nutshell\n",
      "\n",
      "| **What** | **How it works** | **Why it matters** |\n",
      "|----------|------------------|--------------------|\n",
      "| **Wave‑particle duality** | Light and matter behave like both particles and waves. A photon can travel as a stream of quanta *and* produce an interference pattern like waves. | Explains phenomena that classical physics cannot (e.g., diffraction of electrons). |\n",
      "| **Quantization** | Certain properties (energy, angular momentum) can only take discrete values, not a continuous range. | Leads to the existence of atomic energy levels, the emission/absorption spectra of atoms. |\n",
      "| **Superposition** | A quantum system can be in *many* states at once. The system’s state is described by a wavefunction that assigns probabilities to each outcome. | Gives rise to interference effects and the possibility of parallel computation in quantum computers. |\n",
      "| **Uncertainty principle** | Some pairs of observables (e.g., position \\(x\\) and momentum \\(p\\)) cannot be known simultaneously with arbitrary precision: \\(\\Delta x\\,\\Delta p \\ge \\hbar/2\\). | Limits how precisely we can predict the behavior of microscopic systems; underpins the stability of atoms. |\n",
      "| **Measurement / Collapse** | When we observe a quantum system, the wavefunction “collapses” to one of its possible eigenstates, with a probability given by the squared amplitude. | Connects the probabilistic formalism to the definite outcomes we see in experiments. |\n",
      "| **Entanglement** | Two or more particles can share a joint state so that measurement on one instantaneously affects the state of the other, no matter the distance. | Key resource for quantum communication, teleportation, and quantum cryptography. |\n",
      "\n",
      "### The Core Mathematical Tool\n",
      "\n",
      "- **Schrödinger Equation**  \n",
      "  \\[\n",
      "  i\\hbar \\frac{\\partial}{\\partial t}\\Psi(\\mathbf{r},t)=\\hat{H}\\Psi(\\mathbf{r},t)\n",
      "  \\]\n",
      "  This equation tells us how the wavefunction \\(\\Psi\\) evolves over time. The Hamiltonian \\(\\hat{H}\\) encodes the system’s energy (kinetic + potential). For a free particle, the solutions are plane waves; for a particle in a potential well, the solutions are standing waves with discrete energies.\n",
      "\n",
      "- **Expectation Values**  \n",
      "  The average value of an observable \\(A\\) is \\(\\langle A\\rangle = \\int \\Psi^* \\hat{A}\\Psi\\,d^3r\\). This is the “classical” reading of a quantum measurement before collapse.\n",
      "\n",
      "### Conceptual Picture\n",
      "\n",
      "1. **Prepare** a system in a known state (e.g., an electron in a hydrogen atom).\n",
      "2. **Let it evolve** under the Schrödinger equation; the system’s state becomes a superposition of many possibilities.\n",
      "3. **Measure** an observable (e.g., position). The act of measuring forces the system into one of the eigenstates of that observable.\n",
      "4. **Repeat** the experiment many times to build up a probability distribution that matches the square of the wavefunction’s amplitude.\n",
      "\n",
      "### Why It’s Not “Just Another Theory”\n",
      "\n",
      "- **Predictive Power**: Quantum mechanics predicts the outcomes of experiments (e.g., the electron diffraction pattern, the energy levels of the hydrogen atom) with extraordinary precision.\n",
      "- **Technological Impact**: Semiconductors, lasers, MRI, and emerging quantum computers all rely on quantum principles.\n",
      "- **Philosophical Insight**: It forces us to rethink reality—information, measurement, and the role of the observer.\n",
      "\n",
      "In short, quantum mechanics tells us that on the microscopic scale, nature is governed by probabilities, discrete energy packets, and waves that can interfere even with single particles. It’s a departure from classical determinism but one that has proved to be one of the most accurate descriptions of the physical world.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9daad078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysisWe need to interpret the question: \"how much memory to reserve for gpt oss on a 4090 rtx gpu\". GPT OSS likely refers to GPT-Chat open-source? Or GPT on GPU? Maybe the user wants to know how much GPU VRAM to reserve for GPT (like a large language model) on an RTX 4090. They might be asking about memory usage for the GPU when running GPT models (like GPT-4, GPT-3.5) on a 4090, and how much memory to set aside/reserve for the library. Or maybe \"GPT OSS\" refers to \"gpt-oss\" or \"gpt-oss\" open source version? Might be referencing \"GPT-OSS\" like GPT-3 open source? But likely the user wants to know how much memory they need to allocate for running a GPT model on an RTX 4090. The RTX 4090 has 24GB of GDDR6X memory. So how much memory to reserve? It depends on model size (e.g., GPT-2, GPT-3, GPT-4). For GPT-2 1.5B, you can run with ~4-5GB. GPT-3 175B would need huge memory, not possible. GPT-4? The largest open source GPT model? Might be GPT-NeoX 20B? That would need ~16GB? Let's think.\n",
      "\n",
      "The user might be using the \"gpt-oss\" library. Maybe they're using \"gpt-oss\" on a 4090. They ask: \"how much memory to reserve for gpt oss on a 4090 rtx gpu\". Likely they want to know how much VRAM to leave for the GPU for the model, or how much to allocate. The answer: The RTX 4090 has 24GB, but the OS and drivers need some. Usually you reserve 1-2GB for drivers, maybe 2-4GB for OS, so you can use up to ~20GB. But memory overhead for the model runtime is significant. GPT-OSS might use the entire memory. So you can set torch.cuda.set_per_process_memory_fraction? Or use torch.cuda.memory_reserved. But the question is ambiguous.\n",
      "\n",
      "We can provide a clear answer: The RTX 4090 has 24GB of VRAM, but you need to leave some for system overhead (~2-4GB). So you can reserve ~20GB for the model. If you use half-precision (FP16), you can fit larger models. For GPT-NeoX 20B, you'll need ~16GB. For GPT-J 6B, ~8GB. For GPT-2 small, ~2GB. So if you want to run GPT-OSS (maybe GPT-4?), you need to consider the memory consumption per token and per batch. Provide approximate numbers.\n",
      "\n",
      "Also mention that you can use torch.cuda.empty_cache to free memory, and use gradient checkpointing. Also mention that you can use swap to CPU but slower.\n",
      "\n",
      "We can also mention that if you want to use the entire GPU, you can set environment variable CUDA_VISIBLE_DEVICES and memory fraction.\n",
      "\n",
      "Thus answer: Reserve about 20GB for GPT-OSS, leaving ~4GB for system. But if you need to run multiple instances, adjust accordingly.\n",
      "\n",
      "We can also explain that memory usage depends on batch size, sequence length, model size. Provide a formula: memory = (sequence_length * batch_size * hidden_size * 2) + overhead. For GPT-OSS, if using 16-bit, overhead ~1-2GB.\n",
      "\n",
      "Let's answer concisely, include suggestions.\n",
      "\n",
      "assistantfinal**Short answer:**  \n",
      "On an RTX 4090 you should leave **≈ 4 – 6 GB** of the 24 GB for the OS, drivers, background processes and any other CUDA libraries you’re using, and then reserve the rest (≈ 18 – 20 GB) for the GPT‑OSS model.  \n",
      "\n",
      "Below is a quick cheat‑sheet that explains why those numbers make sense and how you can fine‑tune the allocation for your particular use‑case.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. What’s on the GPU?\n",
      "\n",
      "| Component | Rough GPU‑VRAM usage | Notes |\n",
      "|-----------|---------------------|-------|\n",
      "| **Operating system + drivers** | 1–2 GB | Modern Windows or Linux + CUDA drivers |\n",
      "| **CUDA runtime / cuBLAS / cuDNN** | 0.5–1 GB | Library code and small helper buffers |\n",
      "| **Model weights** | Depends on model size | FP16 weights: 2 bytes per parameter |\n",
      "| **Activation buffers** | Depends on batch size & seq‑len | Roughly `batch × seq_len × hidden_dim × 2` (forward + backward) |\n",
      "| **Other tensors** | 0.5–1 GB | Gradients, optimizer state,\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"how much memory to reserve for gpt oss on a 4090 rtx gpu\",\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af0e8365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysisWe need to provide Python code example for a simple web server. The user didn't specify which framework or library. We can give a simple example using built-in http.server, or using Flask. Since they said \"simple web server\", maybe use the built-in http.server to keep example minimal. Provide explanation. Also mention how to run it. Provide code that listens on port 8080 and serves static files. Or a minimal Flask server that responds with \"Hello, World!\". It's up to us. Provide code for both? The question is open: \"Can you show me a Python code example for a simple web server?\" So providing a minimal example is fine. Let's show two options: built-in HTTPServer and Flask. Provide both. We'll keep code short. Also mention dependencies. Maybe ask if they want static files or dynamic. But we can provide a single example with built-in library. Let's include the simple HTTPServer example:\n",
      "\n",
      "```\n",
      "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
      "\n",
      "class SimpleHandler(BaseHTTPRequestHandler):\n",
      "    def do_GET(self):\n",
      "        self.send_response(200)\n",
      "        self.send_header('Content-type', 'text/html')\n",
      "        self.end_headers()\n",
      "        self.wfile.write(b\"<html><body><h1>Hello, World!</h1></body></html>\")\n",
      "\n",
      "def run(server_class=HTTPServer, handler_class=SimpleHandler, port=8080):\n",
      "    server_address = ('', port)\n",
      "    httpd = server_class(server_address, handler_class)\n",
      "    print(f'Serving on port {port}...')\n",
      "    httpd.serve_forever()\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    run()\n",
      "```\n",
      "\n",
      "Alternatively, a Flask example:\n",
      "\n",
      "```\n",
      "from flask import Flask\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/')\n",
      "def hello():\n",
      "    return 'Hello, World!'\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(port=8080)\n",
      "```\n",
      "\n",
      "We can provide explanation for each. Provide instructions to run. Also mention how to install flask with pip. Provide some variations. Provide the code. Provide a short explanation. That's it.assistantfinalBelow are two **very‑simple** ways to spin up a web server in Python.  \n",
      "Pick the one that best fits your use‑case (built‑in vs. a tiny framework).\n",
      "\n",
      "---\n",
      "\n",
      "## 1️⃣  Built‑in `http.server` (no external dependencies)\n",
      "\n",
      "```python\n",
      "# simple_http_server.py\n",
      "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
      "\n",
      "class SimpleHandler(BaseHTTPRequestHandler):\n",
      "    \"\"\"Respond to GET requests with a short HTML page.\"\"\"\n",
      "\n",
      "    def do_GET(self):\n",
      "        # Send HTTP status code 200 (OK)\n",
      "        self.send_response(200)\n",
      "\n",
      "        # Tell the client that we’re sending back HTML\n",
      "        self.send_header(\"Content-Type\", \"text/html; charset=utf-8\")\n",
      "        self.end_headers()\n",
      "\n",
      "        # Write a small HTML response\n",
      "        response = \"\"\"\n",
      "        <html>\n",
      "            <head><title>Simple Server</title></head>\n",
      "            <body>\n",
      "                <h1>Hello, World!</h1>\n",
      "                <p>You're on port {port}.</p>\n",
      "            </body>\n",
      "        </html>\n",
      "        \"\"\".format(port=self.server.server_port).encode(\"utf-8\")\n",
      "\n",
      "        self.wfile.write(response)\n",
      "\n",
      "def run(server_class=HTTPServer, handler_class=SimpleHandler, port=8080):\n",
      "    \"\"\"Start the HTTP server.\"\"\"\n",
      "    server_address = (\"\", port)           # \"\" == all interfaces\n",
      "    httpd = server_class(server_address, handler_class)\n",
      "    print(f\"Serving HTTP on port {port}...\")\n",
      "    httpd.serve_forever()\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    run()\n",
      "```\n",
      "\n",
      "### How to run\n",
      "\n",
      "```bash\n",
      "python simple_http_server.py\n",
      "```\n",
      "\n",
      "Open your browser to `http://localhost:8080/` and you’ll see the “Hello, World!” page.\n",
      "\n",
      "---\n",
      "\n",
      "## 2️⃣  Tiny Flask app (one third‑party dependency)\n",
      "\n",
      "If you want a bit more flexibility (routing, templates, etc.) without writing a lot of boilerplate, Flask is a great fit.\n",
      "\n",
      "```bash\n",
      "# Install Flask once\n",
      "pip install flask\n",
      "```\n",
      "\n",
      "```python\n",
      "# simple_flask_app.py\n",
      "from flask import Flask\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route(\"/\")          # URL path\n",
      "def hello():\n",
      "    return \"<h1>Hello, World!</h1>\"\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Run the development server on port 8080\n",
      "    app.run(host=\"0.0.0.0\", port=8080, debug=True)\n",
      "```\n",
      "\n",
      "### How to run\n",
      "\n",
      "```bash\n",
      "python simple_flask_app.py\n",
      "```\n",
      "\n",
      "Navigate to `http://localhost:8080/` and you’ll see the same “Hello, World!” message.\n",
      "\n",
      "---\n",
      "\n",
      "### Which one to use?\n",
      "\n",
      "| Feature | `http.server` | Flask |\n",
      "|---------|---------------|-------|\n",
      "| **Dependencies** | None | `flask` (install\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you show me a Python code example for a simple web server?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "286f38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysisWe need to write PyTorch code for intersection across two sparse tensors. They are defined as tuple of two tensors: first tensor is lengths of values, second tensor is coalesced tensor with all values. The length of first tensor is batch_size.\n",
      "\n",
      "We need to compute intersection across two sparse tensors. Means for each batch element, find common indices? We need to do batched operations, no looping over batch_size. Use vectorized operations.\n",
      "\n",
      "We can think of each sparse representation: lengths: shape (batch_size), values: shape (total_nonzero,...) maybe with indices? Wait \"coalesced tensor with all values\" - maybe they mean the value tensor of the sparse matrix? Actually sparse tensors usually have indices and values. But here they say a tuple: (lengths, coalesced tensor with all values). So they likely store a list of tensors per batch in a flattened representation. For each batch i, length[i] is number of elements, and values is a contiguous tensor of shape (total_len,...). So we need to find intersection of values between two sparse tensors across batch.\n",
      "\n",
      "Assume values are indices? Or values of non-zero entries? Intersection likely on indices.\n",
      "\n",
      "We need to compute intersection of index sets for each batch element.\n",
      "\n",
      "We cannot loop over batch. We can use torch.bucketize or something.\n",
      "\n",
      "Approach: Suppose we have two tuples: (len_a, vals_a) and (len_b, vals_b). We need to produce intersection lengths and values.\n",
      "\n",
      "We can compute cumulative sum of lengths to get start indices. Then we can use torch.unique_intersect? There's torch.intersect1d for 1D. But we need per batch.\n",
      "\n",
      "We can use torch.searchsorted on sorted values. But need to sort each batch's values. We can sort globally by grouping.\n",
      "\n",
      "We can use torch.tensor_split? But no loops.\n",
      "\n",
      "Maybe we can use torch.sort on values along dimension 0, and keep track of batch indices via repeating lengths. Let's create a batch index tensor for each value: batch_idx = torch.repeat_interleave(lengths, lengths). For each batch we have values. Then we can sort by batch_idx and value. Then we can compute intersection by comparing adjacent pairs: For each batch, we need to find values that appear in both sets.\n",
      "\n",
      "We can use torch.cdist? Not efficient.\n",
      "\n",
      "We can create two sets: each value is a pair (batch_idx, value). We can use torch.unique_intersect? There's torch.intersect1d which returns sorted intersection of two 1D tensors. But we need per batch. We can use torch.unique_intersect? Not.\n",
      "\n",
      "Alternative: Use torch.ops.torch.unique_intersect? Not.\n",
      "\n",
      "Simpler: Convert each set into a dictionary mapping batch to set of values, but no loops.\n",
      "\n",
      "We can use torch.cat of two lists of values with labels indicating source. Then compute intersection by sorting by batch and value, then find duplicates that appear twice. Approach:\n",
      "\n",
      "- Create batch indices for A and B: batch_idx_a = torch.repeat_interleave(len_a, len_a)\n",
      "- batch_idx_b similar.\n",
      "\n",
      "- Concatenate: combined = torch.cat([torch.stack([batch_idx_a, vals_a], dim=1), torch.stack([batch_idx_b, vals_b], dim=1)], dim=0)\n",
      "\n",
      "- Sort combined by batch, value.\n",
      "\n",
      "- Then find rows where the same batch and value appears twice consecutively; we can use torch.diff or torch.cumsum.\n",
      "\n",
      "We need to detect duplicates that appear exactly twice, meaning one from each set. But if duplicates appear more than twice if duplicates within same set? Assume values unique within each set? Not guaranteed but we can handle.\n",
      "\n",
      "We can compute a flag: duplicate = ((combined[:-1,0]==combined[1:,0]) & (combined[:-1,1]==combined[1:,1]))\n",
      "\n",
      "Then we need to extract unique intersection elements: For each duplicate pair, we can take one of them.\n",
      "\n",
      "But duplicates may appear multiple times if duplicates within set. But we can deduplicate after.\n",
      "\n",
      "We can use torch.unique on the duplicated pairs to get unique intersections.\n",
      "\n",
      "So code:\n",
      "\n",
      "# Build combined\n",
      "\n",
      "Then get duplicates mask, get indices, then unique.\n",
      "\n",
      "Then we need to split per batch: For each batch, we need lengths and consolidated values.\n",
      "\n",
      "We can get batch_idx_intersect, values_intersect. Then compute lengths per batch via torch.bincount. Then values_intersect flattened.\n",
      "\n",
      "All vectorized.\n",
      "\n",
      "Let's write code. Also need to handle variable dtype.\n",
      "\n",
      "We'll implement a function intersection_sparse(sparse_a, sparse_b). Provide docstring.\n",
      "\n",
      "Let's write.assistantfinalBelow is a **pure‑PyTorch, vectorised** implementation that takes two *sparse* tensors in the form\n",
      "\n",
      "```python\n",
      "(sparse_lengths, sparse_values)\n",
      "```\n",
      "\n",
      "where\n",
      "\n",
      "* `sparse_lengths` – a 1‑D tensor of shape `(batch_size,)` that tells how many\n",
      "  “values” belong to each batch element,\n",
      "* `sparse_values`  – a 2‑D tensor that concatenates all values of all batches\n",
      "  (the values are assumed to be sortable – e.g. integer indices, or any\n",
      "  type that supports the `<` operator).\n",
      "\n",
      "The function returns the intersection of the two sparse tensors **per batch**\n",
      "without any explicit Python loop over the batch dimension.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "\n",
      "def sparse_intersection(\n",
      "    sparse_a: tuple[torch.Tensor, torch.Tensor],\n",
      "    sparse_b: tuple[torch.Tensor, torch.Tensor]\n",
      ") -> tuple[torch.Tensor, torch.Tensor]:\n",
      "    \"\"\"\n",
      "    Compute the element‑wise intersection of two batched sparse tensors.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    sparse_a : tuple (lengths_a, values_a)\n",
      "        * lengths_a : LongTensor of shape (B,)\n",
      "        * values_a  : Tensor of shape (N_a, D) where D is the dimensionality\n",
      "                      of the stored value (e.g. 1 for a single index, or more\n",
      "                      for a composite key).  The values are not required to\n",
      "                      be sorted but must be comparable.\n",
      "\n",
      "    sparse_b : tuple (lengths_b, values_b)\n",
      "        Same format as `sparse_a`.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    (lengths_int, values_int) : tuple\n",
      "        * lengths_int : LongTensor of shape (B,) – number of intersected\n",
      "          elements per batch.\n",
      "        * values_int  : Tensor of shape (N_int, D) – concatenated intersected\n",
      "          values (coalesced across the batch).\n",
      "    \"\"\"\n",
      "    lengths_a, values_a = sparse_a\n",
      "    lengths_b, values_b = sparse_b\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 1. Build a global index for every value that tells to which batch it\n",
      "    #    belongs.  `repeat_interleave` expands the batch indices in the same\n",
      "    #    fashion as the values are laid out in the flattened tensor.\n",
      "    # ------------------------------------------------------------------\n",
      "    batch_idx_a = torch.repeat_interleave(lengths_a, lengths_a)\n",
      "    batch_idx_b = torch.repeat_interleave(lengths_b, lengths_b)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 2. Concatenate the two sets, keeping the batch index next to each value.\n",
      "    # ------------------------------------------------------------------\n",
      "    #   Shape of each part:  (N_a, 1 + D),  (N_b, 1 + D)\n",
      "    part_a = torch.cat([batch_idx_a.unsqueeze(-1), values_a], dim=-1)\n",
      "    part_b = torch.cat([batch_idx_b.unsqueeze(-1), values_b], dim=-1)\n",
      "    combined = torch.cat([part_a, part_b], dim=0)          # (N_a+N_b, 1+D)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 3. Sort the combined tensor first by batch, then by the stored value.\n",
      "    #    This makes duplicates (i.e. intersections) appear consecutively.\n",
      "    # ------------------------------------------------------------------\n",
      "    #  We use stable sort so that the relative order inside each batch is\n",
      "    #  preserved – not strictly necessary but helpful for reproducibility.\n",
      "    combined_sorted, _ = combined.sort(dim=0, stable=True)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 4. Detect duplicate rows that appear at least twice.\n",
      "    #    A duplicate row means that the same `(batch, value)` pair is present\n",
      "    #    in *both* input tensors.  We only care about the pair, not about\n",
      "    #    how many times it appears inside a single set.\n",
      "    # ------------------------------------------------------------------\n",
      "    #  Create a mask that is True where the current row equals the next row.\n",
      "    #  This mask is of length `combined_sorted.shape[0]-1`.\n",
      "    dup_mask = (\n",
      "        (combined_sorted[:-1, 0] == combined_sorted[1:, 0]) &\n",
      "        (combined_sorted[:-1, 1:] == combined_sorted[1:, 1:])\n",
      "    )\n",
      "\n",
      "    #  The intersected rows are the *first* row of each duplicate pair.\n",
      "    #  We pick the rows at indices `idx = torch.nonzero(dup_mask, as_tuple=False).squeeze()`\n",
      "    #  and add 0 to get the first row of the pair (the second row will be at idx+1).\n",
      "    dup_indices = torch.nonzero(dup_mask, as_tuple=False).squeeze()\n",
      "\n",
      "    #  If there are no intersections, return empty tensors of the right shape.\n",
      "    if dup_indices.numel() == 0:\n",
      "        return torch.zeros_like(lengths_a, dtype=torch.long), torch.empty((0, values_a.shape[1]), dtype=values_a.dtype)\n",
      "\n",
      "    #  The intersected rows are at `dup_indices` (first row of each duplicate pair).\n",
      "    intersected_rows = combined_sorted[dup_indices, :]\n",
      "\n",
      "    #  For safety, we remove any accidental duplicates inside the intersection\n",
      "    #  itself (e.g. if a value appeared twice in *both* sets).\n",
      "    #  `torch.unique` keeps the first occurrence and sorts by default.\n",
      "    intersected_rows_unique = torch.unique(intersected_rows, dim=0, sorted=True)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 5. Separate the batch indices from the actual values.\n",
      "    # ------------------------------------------------------------------\n",
      "    batch_idx_int = intersected_rows_unique[:, 0]\n",
      "    values_int   = intersected_rows_unique[:, 1:]\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 6. Compute the length of the intersection for each batch.\n",
      "    # ------------------------------------------------------------------\n",
      "    #  `torch.bincount` works only on 1‑D tensors of type LongTensor.\n",
      "    #  We need to ensure that the max batch index is `batch_size-1`.\n",
      "    lengths_int = torch.bincount(batch_idx_int, minlength=lengths_a.shape[0])\n",
      "\n",
      "    return lengths_int, values_int\n",
      "```\n",
      "\n",
      "### How it works\n",
      "\n",
      "| Step | What happens | Why it is vectorised |\n",
      "|------|--------------|----------------------|\n",
      "| **1** | Build a global batch index per value using `repeat_interleave`. | No Python `for` loop – the whole mapping is done in one call. |\n",
      "| **2** | Concatenate the two sets side‑by‑side. | A single `torch.cat` handles all batches at once. |\n",
      "| **3** | Sort by batch and value. | `torch.sort` is fully vectorised; sorting groups duplicates together. |\n",
      "| **4** | Detect duplicate rows that appear twice (i.e. intersection). | `torch.nonzero` and boolean masks operate on the whole tensor. |\n",
      "| **5** | Extract the unique intersection rows and split batch vs value. | Standard tensor indexing – no loops. |\n",
      "| **6** | Count per‑batch intersection sizes with `torch.bincount`. | `bincount` aggregates across the entire tensor in one shot. |\n",
      "\n",
      "### Example\n",
      "\n",
      "```python\n",
      "# Batch size = 3\n",
      "len_a = torch.tensor([2, 3, 1], dtype=torch.long)\n",
      "val_a = torch.tensor([[1], [3], [2], [4], [5], [6]], dtype=torch.long)\n",
      "\n",
      "len_b = torch.tensor([1, 3, 2], dtype=torch.long)\n",
      "val_b = torch.tensor([[3], [4], [7], [8], [6], [9]], dtype=torch.long)\n",
      "\n",
      "len_int, val_int = sparse_intersection((len_a, val_a), (len_b, val_b))\n",
      "print('lengths of intersection:', len_int)   # tensor([0, 2, 1])\n",
      "print('values of intersection:', val_int)    # tensor([[4], [6], [6]])\n",
      "```\n",
      "\n",
      "The code is fully vectorised, makes no explicit Python loops over the batch dimension, and works with any dtype that supports sorting and comparison. Adjust the dimensionality (`D`) of the `values` tensors accordingly – the implementation handles arbitrary `D`.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write some pytorch code an intersection across two sparse tensors. Sparse tensors are defined as a tuple of 2 tensors : first tensor is lengths of values, the second tensor is coallesced tensor with all values. THe length of first tensor is the batch_size. DO NOT loop over the batch_size. do batched operations as vectorized directly in pytorch\",\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0925aefa-cd03-421a-a4f9-65443a3cab84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Profiler Results (Table) ---\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us        2.154s       406.42%        2.154s        2.154s             1  \n",
      "                                          ProfilerStep*        44.61%     961.194ms       100.00%        2.155s        2.155s       0.000us         0.00%     344.454ms     344.454ms             1  \n",
      "                                           aten::linear         0.60%      12.951ms         4.92%     106.023ms      13.691us       0.000us         0.00%     190.763ms      24.634us          7744  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     163.496ms        30.84%     163.496ms      52.946us          3088  \n",
      "           _matmul_ogs_NNT_bf16xbf16xmxfp4_16x256x128x5         0.00%       0.000us         0.00%       0.000us       0.000us     161.410ms        30.45%     161.410ms      53.376us          3024  \n",
      "                                            aten::addmm         2.76%      59.417ms         3.43%      73.911ms       9.624us     113.007ms        21.32%     113.014ms      14.715us          7680  \n",
      "                                           aten::matmul         0.41%       8.888ms         2.61%      56.164ms      17.551us       0.000us         0.00%      84.964ms      26.551us          3200  \n",
      "                                               aten::mm         0.04%     768.324us         0.04%     910.962us      14.234us      77.749ms        14.67%      77.749ms       1.215ms            64  \n",
      "                                              aten::mul         2.73%      58.803ms         4.38%      94.285ms       4.633us      38.335ms         7.23%      38.340ms       1.884us         20352  \n",
      "                                            aten::copy_         1.50%      32.343ms         2.96%      63.852ms       4.762us      27.394ms         5.17%      27.396ms       2.043us         13408  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      25.132ms         4.74%      25.132ms       5.541us          4536  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      20.791ms         3.92%      20.791ms       1.692us         12288  \n",
      "                                               aten::to         0.25%       5.388ms         3.83%      82.436ms       5.297us       0.000us         0.00%      19.468ms       1.251us         15563  \n",
      "                                         aten::_to_copy         0.58%      12.468ms         3.58%      77.048ms       7.812us       0.000us         0.00%      19.468ms       1.974us          9863  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      15.475ms         2.92%      15.475ms       2.442us          6336  \n",
      "                                              aten::add         1.66%      35.830ms         2.69%      57.992ms       5.299us      13.760ms         2.60%      13.762ms       1.257us         10944  \n",
      "                                              aten::cat         1.30%      27.940ms         1.94%      41.733ms       5.378us      12.829ms         2.42%      12.829ms       1.653us          7760  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      12.260ms         2.31%      12.260ms       1.627us          7536  \n",
      "    _matmul_ogs_NNT_bf16xbf16xmxfp4_16x256x128x1_swiglu         0.00%       0.000us         0.00%       0.000us       0.000us      12.012ms         2.27%      12.012ms     500.495us            24  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      10.578ms         2.00%      10.578ms       3.427us          3087  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.526ms         1.99%      10.526ms       1.142us          9216  \n",
      "                                             aten::mean         0.73%      15.699ms         1.04%      22.384ms       7.138us       8.067ms         1.52%       8.072ms       2.574us          3136  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       8.067ms         1.52%       8.067ms       2.572us          3136  \n",
      "                                            aten::clone         0.18%       3.936ms         1.31%      28.269ms       8.275us       0.000us         0.00%       7.797ms       2.283us          3416  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       7.402ms         1.40%       7.402ms       2.391us          3096  \n",
      "                                          aten::reshape         0.77%      16.671ms         2.41%      51.851ms       2.348us       0.000us         0.00%       7.349ms       0.333us         22080  \n",
      "                                              aten::bmm         1.12%      24.152ms         1.46%      31.457ms      10.031us       7.212ms         1.36%       7.214ms       2.300us          3136  \n",
      "                                          aten::softmax         0.10%       2.048ms         0.48%      10.339ms       6.462us       0.000us         0.00%       7.101ms       4.438us          1600  \n",
      "                                         aten::_softmax         0.24%       5.169ms         0.37%       8.075ms       5.047us       7.101ms         1.34%       7.101ms       4.438us          1600  \n",
      "                                             SortTokens         7.75%     166.927ms         9.94%     214.152ms     139.422us       6.951ms         1.31%       6.955ms       4.528us          1536  \n",
      "                                              aten::sub         0.68%      14.566ms         1.08%      23.315ms       4.792us       6.469ms         1.22%       6.540ms       1.344us          4865  \n",
      "           _matmul_ogs_NNT_bf16xbf16xmxfp4_16x256x128x1         0.00%       0.000us         0.00%       0.000us       0.000us       6.242ms         1.18%       6.242ms     260.069us            24  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       5.948ms         1.12%       5.948ms       1.936us          3072  \n",
      "                                              aten::max         0.49%      10.596ms         0.65%      13.905ms       8.356us       5.931ms         1.12%       5.931ms       3.565us          1664  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.470ms         1.03%       5.470ms       3.561us          1536  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       4.722ms         0.89%       4.722ms       1.506us          3136  \n",
      "void at::native::(anonymous namespace)::cunn_SoftMax...         0.00%       0.000us         0.00%       0.000us       0.000us       4.336ms         0.82%       4.336ms      67.752us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.751ms         0.71%       3.751ms       1.149us          3264  \n",
      "                                            aten::rsqrt         0.44%       9.483ms         0.69%      14.865ms       4.740us       3.674ms         0.69%       3.674ms       1.172us          3136  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.674ms         0.69%       3.674ms       1.172us          3136  \n",
      "                                              aten::pow         0.67%      14.347ms         0.97%      21.003ms       6.697us       3.624ms         0.68%       3.624ms       1.156us          3136  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.624ms         0.68%       3.624ms       1.156us          3136  \n",
      "                                             aten::topk         0.08%       1.659ms         0.17%       3.757ms      58.710us       3.612ms         0.68%       3.612ms      56.436us            64  \n",
      "                          _finalize_matmul_scatter_fp32         0.00%       0.000us         0.00%       0.000us       0.000us       3.558ms         0.67%       3.558ms       2.353us          1512  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.370ms         0.64%       3.370ms       1.074us          3136  \n",
      "                              _combined_routing_compute         0.00%       0.000us         0.00%       0.000us       0.000us       3.017ms         0.57%       3.017ms       1.964us          1536  \n",
      "                                      aten::multinomial         0.04%     885.989us         0.32%       6.889ms     107.644us       0.000us         0.00%       2.436ms      38.065us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us       2.334ms         0.44%       2.334ms       1.835us          1272  \n",
      "                           _finalize_matmul_fp32_swiglu         0.00%       0.000us         0.00%       0.000us       0.000us       2.284ms         0.43%       2.284ms       1.511us          1512  \n",
      "                                                   TopK         4.59%      98.905ms         5.38%     116.025ms      75.537us       2.276ms         0.43%       2.276ms       1.482us          1536  \n",
      "                                          _topk_forward         0.00%       0.000us         0.00%       0.000us       0.000us       2.276ms         0.43%       2.276ms       1.482us          1536  \n",
      "                                    _sum_bitmatrix_rows         0.00%       0.000us         0.00%       0.000us       0.000us       1.983ms         0.37%       1.983ms       1.291us          1536  \n",
      "                               _combined_routing_memset         0.00%       0.000us         0.00%       0.000us       0.000us       1.952ms         0.37%       1.952ms       1.271us          1536  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.936ms         0.37%       1.936ms       2.180us           888  \n",
      "void at::native::mbtopk::computeBlockwiseWithinKCoun...         0.00%       0.000us         0.00%       0.000us       0.000us       1.928ms         0.36%       1.928ms       7.530us           256  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.758ms         0.33%       1.758ms       1.144us          1536  \n",
      "void gemvNSP_kernel<__nv_bfloat16, __nv_bfloat16, __...         0.00%       0.000us         0.00%       0.000us       0.000us       1.711ms         0.32%       1.711ms       2.743us           624  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.585ms         0.30%       1.585ms       2.131us           744  \n",
      "                                      aten::bitwise_and         0.21%       4.419ms         0.67%      14.438ms      14.100us     688.711us         0.13%       1.379ms       1.346us          1024  \n",
      "                                            aten::index         0.16%       3.461ms         0.61%      13.215ms      41.428us     460.256us         0.09%       1.117ms       3.502us           319  \n",
      "void gemv2T_kernel_val<int, int, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us       1.014ms         0.19%       1.014ms       2.112us           480  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_bf16_s1681...         0.00%       0.000us         0.00%       0.000us       0.000us     896.710us         0.17%     896.710us      37.363us            24  \n",
      "                                           aten::arange         0.04%     953.702us         0.19%       4.094ms       5.331us     355.655us         0.07%     711.310us       0.926us           768  \n",
      "ampere_bf16_s16816gemm_bf16_64x64_ldg8_relu_f2f_stag...         0.00%       0.000us         0.00%       0.000us       0.000us     709.510us         0.13%     709.510us      29.563us            24  \n",
      "                                            aten::fill_         0.06%       1.257ms         0.12%       2.647ms       4.221us     617.738us         0.12%     617.738us       0.985us           627  \n",
      "                                          aten::__and__         0.02%     414.384us         0.39%       8.314ms      18.559us       0.000us         0.00%     610.342us       1.362us           448  \n",
      "void at::native::mbtopk::radixFindKthValues<float, u...         0.00%       0.000us         0.00%       0.000us       0.000us     504.677us         0.10%     504.677us       1.971us           256  \n",
      "                                            aten::where         0.05%       1.033ms         0.20%       4.261ms      16.646us     184.353us         0.03%     492.835us       1.925us           256  \n",
      "                   VmappedVmappedVmappedVmappedModIndex         1.34%      28.822ms         1.67%      36.034ms     281.515us       0.000us         0.00%     487.077us       3.805us           128  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     431.636us         0.08%     431.636us       1.635us           264  \n",
      "                                           aten::argmax         0.01%     306.879us         0.02%     498.366us       7.787us     431.461us         0.08%     431.461us       6.742us            64  \n",
      "void cutlass::Kernel2<cutlass_80_wmma_tensorop_bf16_...         0.00%       0.000us         0.00%       0.000us       0.000us     413.250us         0.08%     413.250us       2.152us           192  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     387.172us         0.07%     387.172us       6.050us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us     375.871us         0.07%     375.871us       1.171us           321  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     374.633us         0.07%     374.633us       0.973us           385  \n",
      "                                             aten::isin         0.02%     385.319us         0.08%       1.788ms      27.514us       0.000us         0.00%     374.177us       5.757us            65  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     355.719us         0.07%     355.719us       1.108us           321  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us     355.655us         0.07%     355.655us       0.926us           384  \n",
      "                                               aten::le         0.06%       1.228ms         0.13%       2.889ms      11.285us     174.990us         0.03%     349.980us       1.367us           256  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     333.734us         0.06%     333.734us       3.476us            96  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     317.894us         0.06%     317.894us       1.252us           254  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     317.148us         0.06%     317.148us       2.478us           128  \n",
      "                                               aten::eq         0.04%     923.693us         0.06%       1.382ms       5.379us     315.561us         0.06%     315.561us       1.228us           257  \n",
      "                                              aten::min         0.02%     384.633us         0.03%     689.776us      10.778us     314.562us         0.06%     314.562us       4.915us            64  \n",
      "ampere_bf16_s16816gemm_bf16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us     311.172us         0.06%     311.172us       6.483us            48  \n",
      "                                              aten::sum         0.02%     375.573us         0.03%     567.470us       8.867us     305.536us         0.06%     305.536us       4.774us            64  \n",
      "void at_cuda_detail::cub::DeviceScanByKeyKernel<at_c...         0.00%       0.000us         0.00%       0.000us       0.000us     302.244us         0.06%     302.244us       2.361us           128  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 32, 4, f...         0.00%       0.000us         0.00%       0.000us       0.000us     301.057us         0.06%     301.057us       4.704us            64  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     278.625us         0.05%     278.625us       4.354us            64  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     273.634us         0.05%     273.634us       4.276us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     266.691us         0.05%     266.691us       1.375us           194  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     264.866us         0.05%     264.866us       4.139us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     259.662us         0.05%     259.662us       1.352us           192  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     257.224us         0.05%     257.224us       0.630us           408  \n",
      "                                    aten::_assert_async         0.01%     150.302us         0.02%     355.290us       2.776us     256.739us         0.05%     256.739us       2.006us           128  \n",
      "void at::native::_assert_async_cuda_kernel<bool>(boo...         0.00%       0.000us         0.00%       0.000us       0.000us     256.739us         0.05%     256.739us       2.006us           128  \n",
      "void cutlass::Kernel2<cutlass_80_wmma_tensorop_bf16_...         0.00%       0.000us         0.00%       0.000us       0.000us     250.055us         0.05%     250.055us       2.605us            96  \n",
      "                                         aten::new_ones         0.08%       1.733ms         0.16%       3.410ms      13.319us       0.000us         0.00%     249.290us       0.974us           256  \n",
      "void at::native::mbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     246.563us         0.05%     246.563us       3.853us            64  \n",
      "                                              aten::div         0.02%     475.199us         0.03%     727.289us       5.682us     242.915us         0.05%     242.915us       1.898us           128  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.155s\n",
      "Self CUDA time total: 530.067ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain how neural networks process text data.\"},\n",
    "]\n",
    "\n",
    "\n",
    "prof = torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=1, repeat=1),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./tensorboard\"),\n",
    "    # used when outputting for tensorboard\n",
    ")\n",
    "prof.start()\n",
    "\n",
    "for i in range(5):\n",
    "    # Run the model inference\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=64,  # Using smaller token count for profiling\n",
    "    )\n",
    "    prof.step()\n",
    "prof.stop()\n",
    "print(\"\\n--- Profiler Results (Table) ---\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
