{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9e32db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep  2 06:46:31 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   42C    P5              40W / 450W |    583MiB / 24564MiB |     11%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2037      G   /usr/lib/xorg/Xorg                          226MiB |\n",
      "|    0   N/A  N/A      2262      G   /usr/bin/gnome-shell                        103MiB |\n",
      "|    0   N/A  N/A      6022      G   ...seed-version=20250831-180037.590000      129MiB |\n",
      "|    0   N/A  N/A     56293      G   /usr/share/code/code                        107MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "748108ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0cfe5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61143a64fe2249939d60dc4ab20bd3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 40 files:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91140515f319472f9d80e2e57e53b9ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_id = \"openai/gpt-oss-20b\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cbfcc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Sep  2 06:46:37 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.247.01             Driver Version: 535.247.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off | 00000000:01:00.0  On |                  Off |\n",
      "|  0%   44C    P2              80W / 450W |  14144MiB / 24564MiB |      5%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2037      G   /usr/lib/xorg/Xorg                          226MiB |\n",
      "|    0   N/A  N/A      2262      G   /usr/bin/gnome-shell                        103MiB |\n",
      "|    0   N/A  N/A      6022      G   ...seed-version=20250831-180037.590000      131MiB |\n",
      "|    0   N/A  N/A     56293      G   /usr/share/code/code                         83MiB |\n",
      "|    0   N/A  N/A    104935      C   ...ma/anaconda3/envs/models/bin/python    13576MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea90245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GptOssForCausalLM(\n",
      "  (model): GptOssModel(\n",
      "    (embed_tokens): Embedding(201088, 2880, padding_idx=199999)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x GptOssDecoderLayer(\n",
      "        (self_attn): GptOssAttention(\n",
      "          (q_proj): Linear(in_features=2880, out_features=4096, bias=True)\n",
      "          (k_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=2880, out_features=512, bias=True)\n",
      "          (o_proj): Linear(in_features=4096, out_features=2880, bias=True)\n",
      "        )\n",
      "        (mlp): GptOssMLP(\n",
      "          (router): GptOssTopKRouter()\n",
      "          (experts): Mxfp4GptOssExperts()\n",
      "        )\n",
      "        (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "        (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): GptOssRMSNorm((2880,), eps=1e-05)\n",
      "    (rotary_emb): GptOssRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2880, out_features=201088, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(pipe.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e352184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([201088, 2880]) torch.bfloat16\n",
      "model.layers.0.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.0.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.0.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.0.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.0.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.0.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.0.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.0.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.0.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.0.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.1.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.1.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.1.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.1.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.1.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.1.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.1.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.1.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.1.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.1.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.2.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.2.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.2.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.2.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.2.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.2.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.2.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.2.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.2.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.2.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.3.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.3.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.3.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.3.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.3.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.3.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.3.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.3.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.3.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.3.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.4.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.4.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.4.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.4.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.4.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.4.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.4.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.4.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.4.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.4.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.5.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.5.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.5.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.5.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.5.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.5.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.5.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.5.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.5.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.5.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.6.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.6.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.6.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.6.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.6.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.6.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.6.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.6.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.6.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.6.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.7.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.7.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.7.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.7.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.7.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.7.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.7.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.7.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.7.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.7.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.8.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.8.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.8.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.8.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.8.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.8.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.8.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.8.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.8.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.8.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.9.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.9.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.9.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.9.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.9.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.9.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.9.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.9.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.9.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.9.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.10.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.10.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.10.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.10.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.10.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.10.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.10.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.10.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.10.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.10.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.11.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.11.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.11.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.11.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.11.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.11.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.11.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.11.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.11.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.11.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.12.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.12.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.12.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.12.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.12.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.12.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.12.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.12.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.12.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.12.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.13.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.13.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.13.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.13.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.13.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.13.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.13.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.13.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.13.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.13.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.14.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.14.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.14.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.14.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.14.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.14.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.14.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.14.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.14.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.14.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.15.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.15.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.15.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.15.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.15.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.15.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.15.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.15.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.15.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.15.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.16.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.16.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.16.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.16.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.16.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.16.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.16.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.16.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.16.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.16.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.17.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.17.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.17.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.17.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.17.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.17.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.17.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.17.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.17.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.17.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.18.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.18.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.18.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.18.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.18.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.18.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.18.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.18.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.18.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.18.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.19.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.19.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.19.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.19.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.19.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.19.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.19.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.19.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.19.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.19.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.20.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.20.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.20.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.20.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.20.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.20.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.20.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.20.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.20.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.20.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.21.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.21.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.21.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.21.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.21.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.21.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.21.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.21.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.21.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.21.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.22.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.22.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.22.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.22.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.22.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.22.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.22.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.22.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.22.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.22.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.23.self_attn.sinks torch.Size([64]) torch.bfloat16\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([4096, 2880]) torch.bfloat16\n",
      "model.layers.23.self_attn.q_proj.bias torch.Size([4096]) torch.bfloat16\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.23.self_attn.k_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([512, 2880]) torch.bfloat16\n",
      "model.layers.23.self_attn.v_proj.bias torch.Size([512]) torch.bfloat16\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([2880, 4096]) torch.bfloat16\n",
      "model.layers.23.self_attn.o_proj.bias torch.Size([2880]) torch.bfloat16\n",
      "model.layers.23.mlp.router.weight torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.23.mlp.router.bias torch.Size([32]) torch.bfloat16\n",
      "model.layers.23.mlp.experts.gate_up_proj_bias torch.Size([32, 5760]) torch.bfloat16\n",
      "model.layers.23.mlp.experts.down_proj_bias torch.Size([32, 2880]) torch.bfloat16\n",
      "model.layers.23.input_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([2880]) torch.bfloat16\n",
      "model.norm.weight torch.Size([2880]) torch.bfloat16\n",
      "lm_head.weight torch.Size([201088, 2880]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "parameters_state_dict = pipe.model.state_dict()\n",
    "for key, value in parameters_state_dict.items():\n",
    "    print(key, value.size(), value.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5296cf8e-8893-4e8e-a674-a4ab24cf9d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysisWe need to explain quantum mechanics clearly and concisely. Should cover core principles: wave-particle duality, superposition, uncertainty principle, quantization, measurement, wavefunction, Schrödinger equation, entanglement, etc. Keep it concise, but thorough enough. Probably use analogies. Also maybe mention applications. Use simple language. Provide structure: introduction, key concepts, key equations, interpretation, conclusion. Probably aim for ~500-700 words. Keep it understandable.assistantfinal**Quantum Mechanics in a nutshell**\n",
      "\n",
      "| What | Why it matters | How we see it |\n",
      "|------|----------------|---------------|\n",
      "| **Particles are also waves** | Explains diffraction, interference, and the “spooky” behavior of atoms. | A single electron shot through a double‑slit creates an interference pattern, just like water waves. |\n",
      "| **Superposition** | A quantum system can be in many states at once until we look. | A single photon can be in two places at once; only when measured does it “choose” a position. |\n",
      "| **Quantization** | Energy, angular momentum, and other properties come in discrete packets. | The electron in a hydrogen atom can only sit in specific energy shells, not anywhere in between. |\n",
      "| **Uncertainty** | Precise knowledge of one property limits our knowledge of another. | You can’t know both the exact position *and* momentum of an electron. |\n",
      "| **Wavefunction & Schrödinger equation** | The wavefunction \\( \\psi(x,t) \\) encodes all that can be known; its evolution is governed by Schrödinger’s equation. | \\( i\\hbar \\frac{\\partial \\psi}{\\partial t} = \\hat H \\psi \\). It tells us how probability clouds change over time. |\n",
      "| **Measurement collapses the wavefunction** | Observing a system forces it into one of the possible outcomes, with probabilities given by \\(|\\psi|^2\\). | A photon hitting a screen is detected at one spot; the probability of that spot is determined by the squared amplitude. |\n",
      "| **Entanglement** | Correlations between distant particles are stronger than any classical “spooky action at a distance” would allow. | Two photons produced together have linked polarizations: measuring one instantly tells you the other, no matter the distance. |\n",
      "\n",
      "---\n",
      "\n",
      "### 1. The Core Idea\n",
      "\n",
      "Imagine a particle (electron, photon, atom) as a little cloud of probability. Rather than a point moving along a definite path, the particle is described by a *wavefunction* \\(\\psi\\). The square of its magnitude, \\(|\\psi|^2\\), gives the probability density of finding the particle at a particular location when you measure it. Unlike classical waves, quantum waves are never directly observable; only their statistical predictions are.\n",
      "\n",
      "---\n",
      "\n",
      "### 2. Wave–Particle Duality\n",
      "\n",
      "- **Wave aspect**: The wavefunction obeys the superposition principle. Multiple wavefunctions can add together, creating interference patterns (double‑slit experiment).  \n",
      "- **Particle aspect**: When a measurement occurs, the wavefunction “collapses” to a single outcome—a particle is detected at a particular point.\n",
      "\n",
      "---\n",
      "\n",
      "### 3. Superposition & Interference\n",
      "\n",
      "A quantum system can be in a linear combination of states:\n",
      "\n",
      "\\[\n",
      "|\\Psi\\rangle = c_1|A\\rangle + c_2|B\\rangle\n",
      "\\]\n",
      "\n",
      "When we measure, we only ever see state \\(|A\\rangle\\) or \\(|B\\rangle\\), but the probabilities \\( |c_1|^2 \\) and \\( |c_2|^2 \\) come from the wavefunction’s interference. This is why a single electron creates an interference pattern after passing through two slits—each “path” interferes with the other.\n",
      "\n",
      "---\n",
      "\n",
      "### 4. Quantization\n",
      "\n",
      "In many systems, the allowed energy levels are discrete. For example, the hydrogen atom’s energy levels are:\n",
      "\n",
      "\\[\n",
      "E_n = -\\frac{13.6\\text{ eV}}{n^2}\\quad (n=1,2,3,\\dots)\n",
      "\\]\n",
      "\n",
      "The electron can’t have an energy between \\(E_1\\) and \\(E_2\\). This explains why atoms absorb and emit photons at specific frequencies.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Heisenberg’s Uncertainty Principle\n",
      "\n",
      "It’s impossible to simultaneously know a particle’s position \\(x\\) and momentum \\(p\\) with arbitrary precision:\n",
      "\n",
      "\\[\n",
      "\\Delta x\\, \\Delta p \\ge \\frac{\\hbar}{2}\n",
      "\\]\n",
      "\n",
      "This isn’t due to experimental imperfections; it’s a fundamental property of quantum systems. Think of \\(\\psi\\) as a spread-out wave: narrowing it in position space spreads it out in momentum space, and vice versa.\n",
      "\n",
      "---\n",
      "\n",
      "### 6. The Schrödinger Equation\n",
      "\n",
      "The time‑dependent Schrödinger equation is the “law of\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum mechanics clearly and concisely.\"},\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9daad078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysisThe question: \"how much memory to reserve for gpt oss on a 4090 rtx gpu\". They likely refer to GPT-OSS? Maybe \"GPT-OSS\" is an open source version of GPT? They want to know how much memory to reserve on a RTX 4090 GPU. They might be asking about memory allocation for GPT models, maybe for inference or training. The RTX 4090 has 24GB of GDDR6X memory. They might want to know how much memory to reserve for GPT-OSS? They might be on a server with multiple GPU usage. They might need to reserve some memory for system overhead. They might want to know \"how much memory to reserve for GPT-OSS\" i.e., how much memory will GPT-OSS use? Perhaps they want to know how many tokens per batch, etc.\n",
      "\n",
      "Need to answer: The memory usage depends on model size, batch size, sequence length, precision (FP16, BF16, INT8, etc.), caching for attention, etc. For GPT-OSS (maybe GPT-2 or GPT-3?). GPT-OSS might refer to \"GPT Open Source\" like GPT-Neo, GPT-J, GPT-NeoX, GPT-4? But GPT-4 is not open source. GPT-OSS might be a hypothetical open source implementation. I think they refer to \"GPT-OSS\" from OpenAI? There is \"GPT-OSS\" from the open source community. Alternatively, they might refer to \"GPT-OSS\" as \"GPT-Open Source Software\" or \"GPT-Open Source Service.\" But no known product.\n",
      "\n",
      "The question: \"how much memory to reserve for gpt oss on a 4090 rtx gpu\". So they want to know memory required for GPT-OSS inference. Provide guidelines: For GPT-2 (1.5B), in FP16, about 4-5GB of VRAM. For GPT-3 (175B), obviously impossible on 4090. For GPT-J (2.7B) about 5-6GB. For GPT-NeoX 20B, about 20GB. For GPT-4, can't run.\n",
      "\n",
      "But maybe GPT-OSS is a new model. Let's search memory: GPT-OSS could be a new open source GPT model by OpenAI? Actually, \"gpt-oss\" might be a project on GitHub. But I don't recall. We cannot search the internet. But maybe they refer to \"gpt-oss\" which is a variant of GPT-3 that uses 65B parameters. That would require >200GB.\n",
      "\n",
      "Better to answer generically: GPT-OSS is model-agnostic. The memory usage depends on model size and precision. On RTX 4090, you can run up to maybe 32B in FP16 with 24GB. But you must reserve some memory for other processes and for PyTorch overhead, so maybe 2GB. So you can allocate about 22GB for the model.\n",
      "\n",
      "Also mention that you should monitor memory usage with nvidia-smi. Provide approximate memory usage per token: 0.4GB for GPT-2, 1GB for GPT-3. But memory usage can be dynamic due to caching.\n",
      "\n",
      "We can propose a table: For GPT-2 1.5B: ~4GB FP16, ~3.5GB FP32. GPT-3 2.7B: ~8GB FP16. GPT-J 6B: ~12GB. GPT-NeoX 20B: ~20GB FP16. So on 4090 24GB, you can fit GPT-NeoX 20B if you reserve ~2GB for overhead.\n",
      "\n",
      "Also mention that you can use model parallel or pipeline parallel to split across GPUs. The 4090 can hold only one model, so you might split across multiple GPUs or use a smaller model.\n",
      "\n",
      "Also mention that the user may want to reserve memory for caching, activation, and gradient accumulation.\n",
      "\n",
      "Therefore answer: Roughly 24GB is maximum. Reserve 2-3GB for OS and driver. So max model memory ~20-22GB. That limits to 20B or smaller.\n",
      "\n",
      "Also mention that you can use mixed precision to reduce memory usage.\n",
      "\n",
      "Thus answer: For GPT-OSS, you need to estimate memory by the formula: (hidden_size^2 * batch_size * seq_len * 4 bytes) + overhead. Provide an example.\n",
      "\n",
      "The user might want a simple answer: \"You have 24GB, reserve 2GB for system, so 22GB for the model.\" But they might ask \"how much memory to reserve for GPT OSS on a 4090 RTX GPU\". So I'd answer: \"Depends on model size, precision, batch size, seq length. For GPT-OSS (which is presumably a GPT model), you need to reserve at least XGB.\" Provide a table.\n",
      "\n",
      "Also mention that if you use Hugging Face's accelerate, you can\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"how much memory to reserve for gpt oss on a 4090 rtx gpu\",\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af0e8365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysisWe need to respond with a Python code example for a simple web server. Likely using built-in http.server or Flask or FastAPI. The question is generic: \"Can you show me a Python code example for a simple web server?\" Provide a minimal example. Use built-in http.server for simplicity. Or use Flask for demonstration. Provide explanation. Also maybe mention usage. Provide code block. Provide a brief explanation. That's it.assistantfinalSure! Below are two quick ways to spin up a simple web server in Python:\n",
      "\n",
      "| Approach | What you’ll get | How to run |\n",
      "|----------|-----------------|------------|\n",
      "| **Built‑in `http.server`** | A tiny static file server that serves whatever is in your current directory. | `python3 -m http.server 8000` |\n",
      "| **Flask** | A lightweight web framework that lets you add routes, templates, etc. | `python3 -m flask run` (after installing Flask) |\n",
      "\n",
      "---\n",
      "\n",
      "## 1. Using the built‑in `http.server`\n",
      "\n",
      "```python\n",
      "# server_builtin.py\n",
      "import http.server\n",
      "import socketserver\n",
      "\n",
      "PORT = 8000\n",
      "Handler = http.server.SimpleHTTPRequestHandler\n",
      "\n",
      "with socketserver.TCPServer((\"\", PORT), Handler) as httpd:\n",
      "    print(f\"Serving HTTP on 0.0.0.0 port {PORT} (http://localhost:{PORT}/)...\")\n",
      "    httpd.serve_forever()\n",
      "```\n",
      "\n",
      "**Run it**\n",
      "\n",
      "```bash\n",
      "python3 server_builtin.py\n",
      "```\n",
      "\n",
      "Open `http://localhost:8000/` in a browser – you’ll see the files in the folder where the script lives.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Using Flask\n",
      "\n",
      "First, install Flask (if you don’t have it):\n",
      "\n",
      "```bash\n",
      "pip install flask\n",
      "```\n",
      "\n",
      "Then create a small app:\n",
      "\n",
      "```python\n",
      "# app_flask.py\n",
      "from flask import Flask, jsonify\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route(\"/\")\n",
      "def hello():\n",
      "    return \"Hello, Flask!\"\n",
      "\n",
      "@app.route(\"/api/hello\")\n",
      "def api_hello():\n",
      "    return jsonify(message=\"Hello, API!\")\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Use 0.0.0.0 to bind to all interfaces (useful inside Docker, VMs, etc.)\n",
      "    app.run(host=\"0.0.0.0\", port=5000, debug=True)\n",
      "```\n",
      "\n",
      "**Run it**\n",
      "\n",
      "```bash\n",
      "python3 app_flask.py\n",
      "```\n",
      "\n",
      "Visit `http://localhost:5000/` → “Hello, Flask!”  \n",
      "Visit `http://localhost:5000/api/hello` → JSON response.\n",
      "\n",
      "---\n",
      "\n",
      "### Quick Recap\n",
      "\n",
      "- **`http.server`** – no dependencies, great for static files or quick demos.\n",
      "- **Flask** – adds routing, templates, JSON handling, etc., and is still extremely lightweight.\n",
      "\n",
      "Pick whichever fits your needs! If you want something even more minimal (e.g., using `asyncio` or `aiohttp`), let me know and I can show you that too.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Can you show me a Python code example for a simple web server?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=1024,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "286f38b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analysisWe need to interpret the question: We have two sparse tensors defined as a tuple of 2 tensors: first tensor is lengths of values, second tensor is coalesced tensor with all values. The length of first tensor is batch_size. We need to compute intersection across two sparse tensors. Intersection presumably means element-wise intersection of indices? Since we don't have indices explicitly, only lengths and values? But maybe the sparse tensors represent variable-length sequences of values, like ragged tensors. The intersection across two sparse tensors for each batch element: find common values between the two sequences? So intersection per batch. Need to do vectorized across batch. For each batch index, we have two sequences: values1[b], values2[b], lengths l1[b], l2[b]. We need intersection of sets? Or intersection of index positions? Likely intersection of values.\n",
      "\n",
      "Given we can't loop over batch size, we need to use broadcasting and masking. Approach: pad sequences to max length, then use torch.eq and reduce. But we need to handle variable lengths. We could also use torch.unique on each batch? Might be easier to use torch.nn.functional.pad to pad to max lengths, then use torch.eq and logical and to find matches for each pair? But need to compute intersection per batch.\n",
      "\n",
      "We can compute for each batch: for each pair of values from tensorA[b] and tensorB[b], check equality. That yields a matrix shape (batch, max_len_A, max_len_B). Then we can reduce along both dims to get boolean whether any match. But we need intersection maybe as sorted unique values? Might just return a bool mask of intersection? Let's propose intersection list per batch: gather matching values.\n",
      "\n",
      "We can produce for each batch a tensor of intersection values by iterating over values? But can't loop. We can use torch.eq and mask then use torch.max to find matches.\n",
      "\n",
      "We can get indices of matches: use torch.eq broadcast, then use torch.any along dims.\n",
      "\n",
      "But to gather matching values: we can use torch.where on the equality matrix and then gather values from B? Might produce duplicates. We can then unique them.\n",
      "\n",
      "Simplify: compute intersection per batch as bool mask of values in A that are also in B: For each A value, check if exists in B. That is: For each batch, we check if value in A appears in B: Use torch.eq between A and B expanded, then reduce along last dim to get match per A value. Then use that mask to filter A. That gives intersection values.\n",
      "\n",
      "We need to handle variable lengths: pad A and B to max length, use lengths to mask out padded positions.\n",
      "\n",
      "Implementation:\n",
      "\n",
      "```\n",
      "def sparse_intersection(batch_lengths1, batch_vals1, batch_lengths2, batch_vals2):\n",
      "    max_len1 = batch_lengths1.max()\n",
      "    max_len2 = batch_lengths2.max()\n",
      "    # pad\n",
      "    padded1 = torch.nn.functional.pad(batch_vals1, (0, max_len1 - batch_vals1.size(1)))  # but shape (B, max_len1)\n",
      "    padded2 = torch.nn.functional.pad(batch_vals2, (0, max_len2 - batch_vals2.size(1)))\n",
      "    # create masks\n",
      "    mask1 = torch.arange(max_len1, device=batch_lengths1.device)[None,:] < batch_lengths1[:,None]\n",
      "    mask2 = torch.arange(max_len2, device=batch_lengths2.device)[None,:] < batch_lengths2[:,None]\n",
      "    # equality matrix\n",
      "    eq = (padded1[:,:,None] == padded2[:,None,:])  # shape (B, max_len1, max_len2)\n",
      "    eq = eq & mask1[:,:,None] & mask2[:,None,:]\n",
      "    # for each element in A, check if any match\n",
      "    match_in_B = eq.any(dim=2)  # shape (B, max_len1)\n",
      "    # intersection values from A\n",
      "    inter_vals = padded1 * match_in_B.to(padded1.dtype)\n",
      "    # optionally remove zeros? but zeros may be actual values. Use mask.\n",
      "    inter_vals = torch.where(match_in_B, padded1, torch.zeros_like(padded1))\n",
      "    # length of intersections\n",
      "    inter_lengths = match_in_B.sum(dim=1)\n",
      "    return inter_lengths, inter_vals\n",
      "```\n",
      "\n",
      "Need to handle potential duplicates: we can unique along dim 1 for each batch using torch.unique? But torch.unique cannot operate per batch easily. Could use sorting and diff.\n",
      "\n",
      "But probably fine.\n",
      "\n",
      "We also need ensure no loops. Use vectorized operations.\n",
      "\n",
      "Now produce code with explanation.assistantfinalBelow is a **fully vectorised** implementation that works for a batch of ragged (sparse) tensors.  \n",
      "The sparse tensors are represented as a **tuple** `(lengths, values)`:\n",
      "\n",
      "* `lengths` – a 1‑D tensor of shape `(B,)` that tells how many real elements each batch entry has.  \n",
      "* `values`  – a 2‑D “coalesced” tensor of shape `(B, Lmax)` that contains all the values for the whole batch; the\n",
      "  first `lengths[i]` entries of row `i` are valid, the remaining are padding.\n",
      "\n",
      "The routine below returns the **intersection of the two sets of values** for every batch entry\n",
      "(duplicates are removed, the intersection is sorted).\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "\n",
      "def sparse_intersection(\n",
      "        lengths_a: torch.Tensor,\n",
      "        values_a: torch.Tensor,\n",
      "        lengths_b: torch.Tensor,\n",
      "        values_b: torch.Tensor,\n",
      ") -> tuple[torch.Tensor, torch.Tensor]:\n",
      "    \"\"\"\n",
      "    Compute the intersection of two ragged tensors (sparse tensors) in a fully vectorised way.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    lengths_a : torch.LongTensor, shape (B,)\n",
      "        Lengths of the first sparse tensor.\n",
      "    values_a : torch.Tensor, shape (B, Lmax_a)\n",
      "        Coalesced values of the first sparse tensor.\n",
      "    lengths_b : torch.LongTensor, shape (B,)\n",
      "        Lengths of the second sparse tensor.\n",
      "    values_b : torch.Tensor, shape (B, Lmax_b)\n",
      "        Coalesced values of the second sparse tensor.\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    inter_lengths : torch.LongTensor, shape (B,)\n",
      "        Number of intersection elements for each batch entry.\n",
      "    inter_values : torch.Tensor, shape (B, Lmax_inter)\n",
      "        Sorted, unique intersection values for each batch entry.\n",
      "        ``Lmax_inter`` is the maximum intersection size across the batch.\n",
      "    \"\"\"\n",
      "    B = lengths_a.shape[0]\n",
      "    device = lengths_a.device\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 1. Pad the two value tensors to the same length\n",
      "    # ------------------------------------------------------------------\n",
      "    max_len_a = lengths_a.max().item()\n",
      "    max_len_b = lengths_b.max().item()\n",
      "\n",
      "    # Pad to the max length in each tensor\n",
      "    padded_a = F.pad(\n",
      "        values_a, (0, max_len_a - values_a.size(1)), mode=\"constant\", value=0\n",
      "    )          # shape (B, max_len_a)\n",
      "    padded_b = F.pad(\n",
      "        values_b, (0, max_len_b - values_b.size(1)), mode=\"constant\", value=0\n",
      "    )          # shape (B, max_len_b)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 2. Build masks that ignore padded positions\n",
      "    # ------------------------------------------------------------------\n",
      "    idx = torch.arange(max_len_a, device=device)[None, :]          # (1, max_len_a)\n",
      "    mask_a = idx < lengths_a[:, None]                             # (B, max_len_a)\n",
      "\n",
      "    idx = torch.arange(max_len_b, device=device)[None, :]          # (1, max_len_b)\n",
      "    mask_b = idx < lengths_b[:, None]                             # (B, max_len_b)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 3. Compute a boolean matrix that tells, for every element of A,\n",
      "    #    whether it appears in B\n",
      "    # ------------------------------------------------------------------\n",
      "    # Broadcast A over the third dimension and B over the second dimension\n",
      "    eq = (padded_a[:, :, None] == padded_b[:, None, :])           # (B, max_len_a, max_len_b)\n",
      "    eq = eq & mask_a[:, :, None] & mask_b[:, None, :]             # keep only real positions\n",
      "\n",
      "    # For each element of A, check if there is *any* match in B\n",
      "    present_in_b = eq.any(dim=2)                                  # (B, max_len_a)\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 4. Extract the intersection values (unique & sorted)\n",
      "    # ------------------------------------------------------------------\n",
      "    # Keep only the values from A that are present in B\n",
      "    inter_vals_masked = torch.where(present_in_b, padded_a, torch.full_like(padded_a, 0))\n",
      "\n",
      "    # Remove zeros that were inserted by padding (they are not valid values)\n",
      "    # We will sort & unique per batch later\n",
      "    # ------------------------------------------------------------------\n",
      "    # 5. Make every row sorted and unique\n",
      "    # ------------------------------------------------------------------\n",
      "    # Sort each row\n",
      "    inter_vals_sorted, _ = torch.sort(inter_vals_masked, dim=1)\n",
      "\n",
      "    # For each row, find unique values by looking at differences\n",
      "    # We create a mask that keeps the first occurrence of each value\n",
      "    diff = torch.cat([torch.zeros((B, 1), dtype=inter_vals_sorted.dtype, device=device),\n",
      "                      inter_vals_sorted[:, 1:] - inter_vals_sorted[:, :-1]], dim=1)\n",
      "    unique_mask = diff!= 0\n",
      "    # Also keep the first element of every row\n",
      "    unique_mask[:, 0] = True\n",
      "\n",
      "    # Apply mask\n",
      "    inter_vals_unique = torch.where(unique_mask, inter_vals_sorted, torch.full_like(inter_vals_sorted, 0))\n",
      "\n",
      "    # ------------------------------------------------------------------\n",
      "    # 6. Compute the lengths of the intersection for each batch entry\n",
      "    # ------------------------------------------------------------------\n",
      "    inter_lengths = unique_mask.sum(dim=1)                         # (B,)\n",
      "\n",
      "    return inter_lengths, inter_vals_unique\n",
      "```\n",
      "\n",
      "### How it works\n",
      "\n",
      "| Step | What it does | Why it is vectorised |\n",
      "|------|--------------|----------------------|\n",
      "| **1** | Pad both value tensors to their own maximum length. | `torch.pad` is a single tensor operation. |\n",
      "| **2** | Build masks that mark the real entries in each batch. | A single broadcasted comparison. |\n",
      "| **3** | Compute a 3‑D equality matrix `(B, L_a, L_b)` and reduce over the last axis to know whether each element of `A` appears in `B`. | All broadcasting and reduction are tensor ops – no Python loops. |\n",
      "| **4** | Keep only those values of `A` that are present in `B`. | `torch.where` is vectorised. |\n",
      "| **5** | Sort each row and deduplicate. | `torch.sort` and simple difference operations are vectorised. |\n",
      "| **6** | Count unique elements per batch. | A simple reduction (`sum`). |\n",
      "\n",
      "### Usage example\n",
      "\n",
      "```python\n",
      "# Example batch of 3 entries\n",
      "lengths_a = torch.tensor([3, 2, 4])\n",
      "values_a  = torch.tensor([[5, 1, 9, 0],   # 0 is padding\n",
      "                          [2, 7, 0, 0],\n",
      "                          [3, 3, 8, 6]])\n",
      "\n",
      "lengths_b = torch.tensor([2, 3, 3])\n",
      "values_b  = torch.tensor([[1, 9, 0, 0],\n",
      "                          [7, 2, 4, 0],\n",
      "                          [3, 8, 3, 0]])\n",
      "\n",
      "inter_len, inter_vals = sparse_intersection(lengths_a, values_a,\n",
      "                                            lengths_b, values_b)\n",
      "\n",
      "print(inter_len)   # tensor([2, 2, 2])\n",
      "print(inter_vals)  # tensor([[1, 9, 0, 0],\n",
      "                    #        [2, 7, 0, 0],\n",
      "                    #        [3, 8, 0, 0]])\n",
      "```\n",
      "\n",
      "The returned tensor `inter_vals` is padded with zeros to the maximum intersection size found in the batch.  \n",
      "Feel free to trim that padding if you need a compact list per batch entry.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Write some pytorch code an intersection across two sparse tensors. Sparse tensors are defined as a tuple of 2 tensors : first tensor is lengths of values, the second tensor is coallesced tensor with all values. THe length of first tensor is the batch_size. DO NOT loop over the batch_size. do batched operations as vectorized directly in pytorch\",\n",
    "    },\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=4096,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0925aefa-cd03-421a-a4f9-65443a3cab84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Profiler Results (Table) ---\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.00%       0.000us         0.00%       0.000us       0.000us        2.393s       454.50%        2.393s        2.393s             1  \n",
      "                                          ProfilerStep*        43.40%        1.039s       100.00%        2.394s        2.394s       0.000us         0.00%     342.903ms     342.903ms             1  \n",
      "                                           aten::linear         0.59%      14.097ms         5.77%     138.195ms      17.845us       0.000us         0.00%     196.359ms      25.356us          7744  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us     169.684ms        32.22%     169.684ms      54.950us          3088  \n",
      "           _matmul_ogs_NNT_bf16xbf16xmxfp4_16x256x128x5         0.00%       0.000us         0.00%       0.000us       0.000us     159.971ms        30.38%     159.971ms      52.900us          3024  \n",
      "                                            aten::addmm         2.73%      65.442ms         4.29%     102.784ms      13.383us     117.915ms        22.39%     117.915ms      15.354us          7680  \n",
      "                                           aten::matmul         0.41%       9.758ms         2.57%      61.443ms      19.201us       0.000us         0.00%      85.292ms      26.654us          3200  \n",
      "                                               aten::mm         0.04%     946.080us         0.05%       1.112ms      17.381us      78.444ms        14.90%      78.444ms       1.226ms            64  \n",
      "                                              aten::mul         2.62%      62.605ms         4.14%      99.115ms       4.870us      36.395ms         6.91%      36.396ms       1.788us         20352  \n",
      "                                            aten::copy_         1.52%      36.472ms         3.07%      73.522ms       5.483us      26.159ms         4.97%      26.162ms       1.951us         13408  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us      24.583ms         4.67%      24.583ms       5.420us          4536  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      19.820ms         3.76%      19.820ms       1.613us         12288  \n",
      "                                               aten::to         0.26%       6.138ms         3.93%      94.062ms       6.044us       0.000us         0.00%      18.631ms       1.197us         15563  \n",
      "                                         aten::_to_copy         0.59%      14.129ms         3.67%      87.924ms       8.915us       0.000us         0.00%      18.631ms       1.889us          9863  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us      14.831ms         2.82%      14.831ms       2.341us          6336  \n",
      "                                              aten::add         1.62%      38.847ms         2.57%      61.567ms       5.626us      13.157ms         2.50%      13.160ms       1.203us         10944  \n",
      "                                              aten::cat         1.25%      29.921ms         1.84%      44.133ms       5.687us      12.332ms         2.34%      12.332ms       1.589us          7760  \n",
      "    _matmul_ogs_NNT_bf16xbf16xmxfp4_16x256x128x1_swiglu         0.00%       0.000us         0.00%       0.000us       0.000us      11.974ms         2.27%      11.974ms     498.907us            24  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us      11.787ms         2.24%      11.787ms       1.564us          7536  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      10.039ms         1.91%      10.039ms       1.089us          9216  \n",
      "void at::native::unrolled_elementwise_kernel<at::nat...         0.00%       0.000us         0.00%       0.000us       0.000us       9.911ms         1.88%       9.911ms       3.211us          3087  \n",
      "                                             aten::mean         0.71%      17.024ms         1.03%      24.605ms       7.846us       7.648ms         1.45%       7.650ms       2.439us          3136  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       7.648ms         1.45%       7.648ms       2.439us          3136  \n",
      "                                            aten::clone         0.18%       4.307ms         1.31%      31.277ms       9.156us       0.000us         0.00%       7.407ms       2.168us          3416  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       7.030ms         1.33%       7.030ms       2.271us          3096  \n",
      "                                          aten::reshape         0.74%      17.691ms         2.32%      55.497ms       2.513us       0.000us         0.00%       6.980ms       0.316us         22080  \n",
      "                                              aten::bmm         1.12%      26.842ms         1.45%      34.826ms      11.105us       6.845ms         1.30%       6.848ms       2.184us          3136  \n",
      "                                          aten::softmax         0.09%       2.270ms         0.47%      11.214ms       7.009us       0.000us         0.00%       6.755ms       4.222us          1600  \n",
      "                                         aten::_softmax         0.23%       5.567ms         0.36%       8.672ms       5.420us       6.755ms         1.28%       6.755ms       4.222us          1600  \n",
      "                                             SortTokens         7.43%     177.962ms         9.54%     228.504ms     148.765us       6.623ms         1.26%       6.627ms       4.314us          1536  \n",
      "                                              aten::sub         0.65%      15.664ms         1.02%      24.420ms       5.020us       6.167ms         1.17%       6.234ms       1.281us          4865  \n",
      "           _matmul_ogs_NNT_bf16xbf16xmxfp4_16x256x128x1         0.00%       0.000us         0.00%       0.000us       0.000us       6.066ms         1.15%       6.066ms     252.768us            24  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       5.690ms         1.08%       5.690ms       1.852us          3072  \n",
      "                                              aten::max         0.48%      11.411ms         0.63%      15.078ms       9.061us       5.689ms         1.08%       5.689ms       3.419us          1664  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       5.256ms         1.00%       5.256ms       3.422us          1536  \n",
      "void at::native::elementwise_kernel<128, 2, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us       4.526ms         0.86%       4.526ms       1.443us          3136  \n",
      "void at::native::(anonymous namespace)::cunn_SoftMax...         0.00%       0.000us         0.00%       0.000us       0.000us       4.127ms         0.78%       4.127ms      64.486us            64  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.564ms         0.68%       3.564ms       1.092us          3264  \n",
      "                                             aten::topk         0.09%       2.078ms         0.20%       4.678ms      73.095us       3.522ms         0.67%       3.522ms      55.032us            64  \n",
      "                                            aten::rsqrt         0.43%      10.355ms         0.67%      16.157ms       5.152us       3.508ms         0.67%       3.508ms       1.119us          3136  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.508ms         0.67%       3.508ms       1.119us          3136  \n",
      "                                              aten::pow         0.66%      15.759ms         0.96%      22.904ms       7.303us       3.457ms         0.66%       3.457ms       1.102us          3136  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.457ms         0.66%       3.457ms       1.102us          3136  \n",
      "                          _finalize_matmul_scatter_fp32         0.00%       0.000us         0.00%       0.000us       0.000us       3.385ms         0.64%       3.385ms       2.239us          1512  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.226ms         0.61%       3.226ms       1.029us          3136  \n",
      "                              _combined_routing_compute         0.00%       0.000us         0.00%       0.000us       0.000us       2.868ms         0.54%       2.868ms       1.867us          1536  \n",
      "                                      aten::multinomial         0.05%       1.269ms         0.37%       8.777ms     137.137us       0.000us         0.00%       2.310ms      36.091us            64  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us       2.198ms         0.42%       2.198ms       1.728us          1272  \n",
      "                           _finalize_matmul_fp32_swiglu         0.00%       0.000us         0.00%       0.000us       0.000us       2.176ms         0.41%       2.176ms       1.439us          1512  \n",
      "                                                   TopK         4.48%     107.271ms         5.25%     125.669ms      81.816us       2.162ms         0.41%       2.162ms       1.407us          1536  \n",
      "                                          _topk_forward         0.00%       0.000us         0.00%       0.000us       0.000us       2.162ms         0.41%       2.162ms       1.407us          1536  \n",
      "void at::native::mbtopk::computeBlockwiseWithinKCoun...         0.00%       0.000us         0.00%       0.000us       0.000us       1.916ms         0.36%       1.916ms       7.484us           256  \n",
      "                                    _sum_bitmatrix_rows         0.00%       0.000us         0.00%       0.000us       0.000us       1.887ms         0.36%       1.887ms       1.228us          1536  \n",
      "                               _combined_routing_memset         0.00%       0.000us         0.00%       0.000us       0.000us       1.869ms         0.35%       1.869ms       1.217us          1536  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.820ms         0.35%       1.820ms       2.050us           888  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.681ms         0.32%       1.681ms       1.094us          1536  \n",
      "void gemvNSP_kernel<__nv_bfloat16, __nv_bfloat16, __...         0.00%       0.000us         0.00%       0.000us       0.000us       1.642ms         0.31%       1.642ms       2.632us           624  \n",
      "std::enable_if<!(false), void>::type internal::gemvx...         0.00%       0.000us         0.00%       0.000us       0.000us       1.507ms         0.29%       1.507ms       2.025us           744  \n",
      "                                      aten::bitwise_and         0.21%       5.029ms         0.67%      16.069ms      15.692us     660.592us         0.13%       1.321ms       1.290us          1024  \n",
      "                                            aten::index         0.17%       4.141ms         0.67%      16.059ms      50.341us     430.509us         0.08%       1.054ms       3.305us           319  \n",
      "void gemv2T_kernel_val<int, int, __nv_bfloat16, __nv...         0.00%       0.000us         0.00%       0.000us       0.000us     948.025us         0.18%     948.025us       1.975us           480  \n",
      "void cutlass::Kernel2<cutlass_80_tensorop_bf16_s1681...         0.00%       0.000us         0.00%       0.000us       0.000us     890.679us         0.17%     890.679us      37.112us            24  \n",
      "ampere_bf16_s16816gemm_bf16_64x64_ldg8_relu_f2f_stag...         0.00%       0.000us         0.00%       0.000us       0.000us     701.172us         0.13%     701.172us      29.216us            24  \n",
      "                                           aten::arange         0.05%       1.170ms         0.20%       4.765ms       6.205us     340.071us         0.06%     680.142us       0.886us           768  \n",
      "                                            aten::fill_         0.06%       1.485ms         0.13%       3.098ms       4.942us     589.613us         0.11%     589.613us       0.940us           627  \n",
      "                                          aten::__and__         0.02%     464.339us         0.38%       9.214ms      20.567us       0.000us         0.00%     585.651us       1.307us           448  \n",
      "void at::native::mbtopk::radixFindKthValues<float, u...         0.00%       0.000us         0.00%       0.000us       0.000us     481.457us         0.09%     481.457us       1.881us           256  \n",
      "                                            aten::where         0.05%       1.271ms         0.21%       4.960ms      19.373us     175.049us         0.03%     468.698us       1.831us           256  \n",
      "                   VmappedVmappedVmappedVmappedModIndex         1.30%      31.127ms         1.66%      39.826ms     311.140us       0.000us         0.00%     460.786us       3.600us           128  \n",
      "void (anonymous namespace)::softmax_warp_forward<c10...         0.00%       0.000us         0.00%       0.000us       0.000us     430.247us         0.08%     430.247us       1.630us           264  \n",
      "                                           aten::argmax         0.02%     362.146us         0.02%     597.492us       9.336us     408.264us         0.08%     408.264us       6.379us            64  \n",
      "void cutlass::Kernel2<cutlass_80_wmma_tensorop_bf16_...         0.00%       0.000us         0.00%       0.000us       0.000us     391.204us         0.07%     391.204us       2.038us           192  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     365.706us         0.07%     365.706us       5.714us            64  \n",
      "void at::native::vectorized_elementwise_kernel<2, at...         0.00%       0.000us         0.00%       0.000us       0.000us     358.986us         0.07%     358.986us       1.118us           321  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     357.635us         0.07%     357.635us       0.929us           385  \n",
      "                                             aten::isin         0.02%     506.262us         0.09%       2.180ms      33.532us       0.000us         0.00%     351.410us       5.406us            65  \n",
      "void (anonymous namespace)::elementwise_kernel_with_...         0.00%       0.000us         0.00%       0.000us       0.000us     340.071us         0.06%     340.071us       0.886us           384  \n",
      "                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us     338.409us         0.06%     338.409us       1.054us           321  \n",
      "                                               aten::le         0.06%       1.409ms         0.14%       3.293ms      12.863us     167.173us         0.03%     334.346us       1.306us           256  \n",
      "void at::native::(anonymous namespace)::CatArrayBatc...         0.00%       0.000us         0.00%       0.000us       0.000us     318.537us         0.06%     318.537us       3.318us            96  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     303.812us         0.06%     303.812us       1.196us           254  \n",
      "ampere_bf16_s16816gemm_bf16_64x64_ldg8_f2f_stages_64...         0.00%       0.000us         0.00%       0.000us       0.000us     300.872us         0.06%     300.872us       6.268us            48  \n",
      "                                               aten::eq         0.05%       1.113ms         0.07%       1.675ms       6.517us     299.561us         0.06%     299.561us       1.166us           257  \n",
      "                                              aten::min         0.02%     493.528us         0.04%     863.444us      13.491us     298.885us         0.06%     298.885us       4.670us            64  \n",
      "void at::native::index_elementwise_kernel<128, 4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     297.804us         0.06%     297.804us       2.327us           128  \n",
      "                                              aten::sum         0.02%     459.493us         0.03%     712.113us      11.127us     293.927us         0.06%     293.927us       4.593us            64  \n",
      "void at::native::radixSortKVInPlace<-2, -1, 32, 4, f...         0.00%       0.000us         0.00%       0.000us       0.000us     287.141us         0.05%     287.141us       4.487us            64  \n",
      "void at_cuda_detail::cub::DeviceScanByKeyKernel<at_c...         0.00%       0.000us         0.00%       0.000us       0.000us     286.114us         0.05%     286.114us       2.235us           128  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     259.751us         0.05%     259.751us       4.059us            64  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     257.092us         0.05%     257.092us       4.017us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     255.082us         0.05%     255.082us       1.315us           194  \n",
      "void cutlass::Kernel2<cutlass_80_wmma_tensorop_bf16_...         0.00%       0.000us         0.00%       0.000us       0.000us     254.723us         0.05%     254.723us       2.653us            96  \n",
      "                                        Memset (Device)         0.00%       0.000us         0.00%       0.000us       0.000us     253.984us         0.05%     253.984us       0.623us           408  \n",
      "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us     250.599us         0.05%     250.599us       3.916us            64  \n",
      "void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     248.585us         0.05%     248.585us       1.295us           192  \n",
      "                                    aten::_assert_async         0.01%     212.156us         0.02%     469.803us       3.670us     244.105us         0.05%     244.105us       1.907us           128  \n",
      "void at::native::_assert_async_cuda_kernel<bool>(boo...         0.00%       0.000us         0.00%       0.000us       0.000us     244.105us         0.05%     244.105us       1.907us           128  \n",
      "                                         aten::new_ones         0.09%       2.067ms         0.17%       4.016ms      15.688us       0.000us         0.00%     238.120us       0.930us           256  \n",
      "void at::native::mbtopk::gatherTopK<float, unsigned ...         0.00%       0.000us         0.00%       0.000us       0.000us     235.433us         0.04%     235.433us       3.679us            64  \n",
      "                                              aten::div         0.03%     654.632us         0.04%     981.412us       7.667us     230.822us         0.04%     230.822us       1.803us           128  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.394s\n",
      "Self CUDA time total: 526.606ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain how neural networks process text data.\"},\n",
    "]\n",
    "\n",
    "\n",
    "prof = torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=1, warmup=1, active=1, repeat=1),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(\"./tensorboard\"),\n",
    "    # used when outputting for tensorboard\n",
    ")\n",
    "prof.start()\n",
    "\n",
    "for i in range(5):\n",
    "    # Run the model inference\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=64,  # Using smaller token count for profiling\n",
    "    )\n",
    "    prof.step()\n",
    "prof.stop()\n",
    "print(\"\\n--- Profiler Results (Table) ---\")\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b081f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b7c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
